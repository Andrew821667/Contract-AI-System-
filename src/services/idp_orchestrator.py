# -*- coding: utf-8 -*-
"""
IDP Orchestrator - –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏—è –≤—Å–µ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
–ì–ª–∞–≤–Ω–∞—è —Ç–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –¥–ª—è Intelligent Document Processing
"""
import asyncio
from typing import Dict, Any, Optional, List
from pathlib import Path
from datetime import datetime
from loguru import logger

from ..schemas.idp_schemas import IntermediateJSONSchema, validate_intermediate_json
from ..models.database import get_db
from sqlalchemy.orm import Session


class IDPOrchestrator:
    """
    –ö–æ–æ—Ä–¥–∏–Ω–∏—Ä—É–µ—Ç –≤–µ—Å—å –ø—Ä–æ—Ü–µ—Å—Å IDP –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

    –û—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å:
    1. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    2. –í—ã–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞
    3. –ö–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏—è —ç—Ç–∞–ø–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    4. –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
    5. –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
    """

    def __init__(
        self,
        db_session: Session,
        file_storage=None,
        llm_gateway=None
    ):
        self.db = db_session
        self.storage = file_storage
        self.llm = llm_gateway

        # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –ø–∞–π–ø–ª–∞–π–Ω–∞ (–±—É–¥—É—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏)
        self._classifier = None
        self._layout_analyzer = None
        self._ocr_service = None
        self._entity_extractor = None
        self._schema_mapper = None

    @property
    def classifier(self):
        """Lazy loading DocumentClassifier"""
        if self._classifier is None:
            from .document_classifier import DocumentClassifier
            self._classifier = DocumentClassifier()
        return self._classifier

    @property
    def layout_analyzer(self):
        """Lazy loading LayoutAnalyzer"""
        if self._layout_analyzer is None:
            from .layout_analyzer import LayoutAnalyzer
            self._layout_analyzer = LayoutAnalyzer()
        return self._layout_analyzer

    @property
    def ocr_service(self):
        """Lazy loading EnhancedOCRService"""
        if self._ocr_service is None:
            from .ocr_service import EnhancedOCRService
            self._ocr_service = EnhancedOCRService()
        return self._ocr_service

    @property
    def entity_extractor(self):
        """Lazy loading MultiLevelEntityExtractor"""
        if self._entity_extractor is None:
            from .entity_extractor import MultiLevelEntityExtractor
            self._entity_extractor = MultiLevelEntityExtractor(self.llm)
        return self._entity_extractor

    @property
    def schema_mapper(self):
        """Lazy loading SchemaMapper"""
        if self._schema_mapper is None:
            from .schema_mapper import SchemaMapper
            self._schema_mapper = SchemaMapper(self.db)
        return self._schema_mapper

    async def process_document(
        self,
        contract_id: str,
        file_data: bytes,
        filename: str,
        idp_mode: str = 'auto'
    ) -> Dict[str, Any]:
        """
        –ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞

        Args:
            contract_id: ID –¥–æ–≥–æ–≤–æ—Ä–∞ –≤ –ë–î
            file_data: –ë–∏–Ω–∞—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª–∞
            filename: –ò–º—è —Ñ–∞–π–ª–∞
            idp_mode: –†–µ–∂–∏–º –æ–±—Ä–∞–±–æ—Ç–∫–∏ ('auto', 'fast', 'deep')

        Returns:
            Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        logger.info(f"üöÄ Starting IDP processing for contract {contract_id} (mode: {idp_mode})")
        start_time = datetime.now()

        try:
            # ====== –≠–¢–ê–ü 1: INGESTION & CLASSIFICATION ======
            logger.info(f"üì• Stage 1: Ingestion & Classification")
            stage_start = datetime.now()

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª —Ñ–∞–π–ª–∞
            file_path = self._save_original_file(contract_id, file_data, filename)

            # –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞
            doc_type = self.classifier.classify(file_path)
            self._log_stage(
                contract_id=contract_id,
                stage='classification',
                status='success',
                output_data={
                    'format': doc_type.format,
                    'is_searchable': doc_type.is_searchable,
                    'page_count': doc_type.page_count,
                    'confidence': doc_type.confidence
                },
                duration_ms=(datetime.now() - stage_start).total_seconds() * 1000
            )

            # ====== –≠–¢–ê–ü 2: ROUTE TO PIPELINE ======
            logger.info(f"üîÄ Routing to pipeline: {doc_type.format}")

            if doc_type.format == 'xml':
                intermediate = await self._process_xml(contract_id, file_path)

            elif doc_type.format == 'pdf' and doc_type.is_searchable:
                intermediate = await self._process_searchable_pdf(
                    contract_id, file_path, idp_mode
                )

            elif doc_type.format in ['pdf', 'jpg', 'png']:
                intermediate = await self._process_scanned_document(
                    contract_id, file_path, idp_mode
                )

            else:
                raise ValueError(f"Unsupported format: {doc_type.format}")

            # ====== –≠–¢–ê–ü 5: VALIDATION ======
            logger.info(f"‚úÖ Stage 5: Validation")
            stage_start = datetime.now()

            try:
                validated = validate_intermediate_json(intermediate.dict())
                self._log_stage(
                    contract_id=contract_id,
                    stage='validation',
                    status='success',
                    output_data={'fields_count': len(validated.dict().keys())},
                    duration_ms=(datetime.now() - stage_start).total_seconds() * 1000
                )
            except Exception as validation_error:
                logger.warning(f"‚ö†Ô∏è Validation issues: {validation_error}")
                self._log_stage(
                    contract_id=contract_id,
                    stage='validation',
                    status='partial',
                    output_data={},
                    error_message=str(validation_error),
                    duration_ms=(datetime.now() - stage_start).total_seconds() * 1000
                )
                # –°–æ–∑–¥–∞–µ–º quality issues
                self._create_quality_issues(contract_id, validation_error)

            # ====== –≠–¢–ê–ü 6: STORAGE ======
            logger.info(f"üíæ Stage 6: Storage")
            stage_start = datetime.now()

            core_id = await self.schema_mapper.save_to_database(
                contract_id, intermediate.dict()
            )

            self._log_stage(
                contract_id=contract_id,
                stage='storage',
                status='success',
                output_data={'core_id': core_id},
                duration_ms=(datetime.now() - stage_start).total_seconds() * 1000
            )

            # ====== –ó–ê–í–ï–†–®–ï–ù–ò–ï ======
            total_duration = (datetime.now() - start_time).total_seconds()
            logger.info(f"‚úÖ IDP processing completed for contract {contract_id} in {total_duration:.2f}s")

            return {
                'success': True,
                'contract_id': contract_id,
                'core_id': core_id,
                'duration_sec': total_duration,
                'intermediate_json': intermediate.dict()
            }

        except Exception as e:
            logger.error(f"‚ùå IDP processing failed for contract {contract_id}: {e}")
            import traceback
            traceback.print_exc()

            self._log_stage(
                contract_id=contract_id,
                stage='processing',
                status='failed',
                output_data={},
                error_message=str(e),
                duration_ms=(datetime.now() - start_time).total_seconds() * 1000
            )

            return {
                'success': False,
                'contract_id': contract_id,
                'error': str(e)
            }

    async def _process_xml(
        self,
        contract_id: str,
        file_path: str
    ) -> IntermediateJSONSchema:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ XML –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥)
        """
        logger.info(f"üìÑ Processing XML document: {file_path}")
        stage_start = datetime.now()

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π DocumentParser
        from ..services.document_parser import DocumentParser
        parser = DocumentParser()

        xml_data = parser.parse(file_path)

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º XML ‚Üí Intermediate JSON
        intermediate = self._xml_to_intermediate(xml_data)

        self._log_stage(
            contract_id=contract_id,
            stage='xml_parsing',
            status='success',
            output_data={
                'method': 'deterministic',
                'fields_extracted': len(intermediate.keys())
            },
            duration_ms=(datetime.now() - stage_start).total_seconds() * 1000
        )

        return IntermediateJSONSchema(**intermediate)

    async def _process_searchable_pdf(
        self,
        contract_id: str,
        file_path: str,
        idp_mode: str
    ) -> IntermediateJSONSchema:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ searchable PDF (—Å —Ç–µ–∫—Å—Ç–æ–≤—ã–º —Å–ª–æ–µ–º)
        """
        logger.info(f"üìÉ Processing searchable PDF: {file_path}")

        # ====== –≠–¢–ê–ü 2: LAYOUT ANALYSIS ======
        stage_start = datetime.now()
        logger.info(f"üîç Stage 2: Layout Analysis")

        pages = self._convert_pdf_to_images(file_path)
        blocks = []

        for page_num, page_img in enumerate(pages):
            page_blocks = self.layout_analyzer.segment_document(page_img)
            blocks.extend(page_blocks)

        self._log_stage(
            contract_id=contract_id,
            stage='layout_analysis',
            status='success',
            output_data={
                'pages': len(pages),
                'blocks': len(blocks)
            },
            duration_ms=(datetime.now() - stage_start).total_seconds() * 1000
        )

        # ====== –≠–¢–ê–ü 3: CASCADING EXTRACTION ======
        stage_start = datetime.now()
        logger.info(f"üî¨ Stage 3: Cascading Extraction")

        intermediate = await self.entity_extractor.extract_all(
            blocks, mode=idp_mode
        )

        self._log_stage(
            contract_id=contract_id,
            stage='entity_extraction',
            status='success',
            output_data={
                'method': 'cascading',
                'mode': idp_mode
            },
            duration_ms=(datetime.now() - stage_start).total_seconds() * 1000
        )

        return intermediate

    async def _process_scanned_document(
        self,
        contract_id: str,
        file_path: str,
        idp_mode: str
    ) -> IntermediateJSONSchema:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∫–∞–Ω–∞/—Ñ–æ—Ç–æ (–º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å)
        """
        logger.info(f"üì∏ Processing scanned document: {file_path}")

        pages = self._convert_pdf_to_images(file_path)

        # ====== –≠–¢–ê–ü 2: OCR ======
        stage_start = datetime.now()
        logger.info(f"üëÅÔ∏è Stage 2: OCR")

        ocr_results = []
        for page_num, page_img in enumerate(pages):
            ocr_result = self.ocr_service.extract_text(
                page_img,
                prefer_structure=True  # PaddleOCR
            )
            ocr_results.append(ocr_result)

        avg_confidence = sum(r.confidence for r in ocr_results if r.confidence) / len(ocr_results)

        self._log_stage(
            contract_id=contract_id,
            stage='ocr',
            status='success',
            output_data={
                'pages': len(pages),
                'avg_confidence': avg_confidence,
                'engine': 'paddleocr'
            },
            duration_ms=(datetime.now() - stage_start).total_seconds() * 1000
        )

        # ====== –≠–¢–ê–ü 3: LAYOUT ANALYSIS ======
        stage_start = datetime.now()
        logger.info(f"üîç Stage 3: Layout Analysis")

        blocks = []
        for page_num, page_img in enumerate(pages):
            page_blocks = self.layout_analyzer.segment_document(page_img)

            # –û–±–æ–≥–∞—â–∞–µ–º –±–ª–æ–∫–∏ —Ç–µ–∫—Å—Ç–æ–º –∏–∑ OCR
            for block in page_blocks:
                block.text = self._extract_text_from_ocr(
                    ocr_results[page_num], block.bbox
                )

            blocks.extend(page_blocks)

        self._log_stage(
            contract_id=contract_id,
            stage='layout_analysis',
            status='success',
            output_data={
                'pages': len(pages),
                'blocks': len(blocks)
            },
            duration_ms=(datetime.now() - stage_start).total_seconds() * 1000
        )

        # ====== –≠–¢–ê–ü 4: CASCADING EXTRACTION ======
        stage_start = datetime.now()
        logger.info(f"üî¨ Stage 4: Cascading Extraction")

        intermediate = await self.entity_extractor.extract_all(
            blocks, mode=idp_mode
        )

        self._log_stage(
            contract_id=contract_id,
            stage='entity_extraction',
            status='success',
            output_data={
                'method': 'cascading',
                'mode': idp_mode
            },
            duration_ms=(datetime.now() - stage_start).total_seconds() * 1000
        )

        return intermediate

    # ============================================================
    # Helper Methods
    # ============================================================

    def _save_original_file(
        self,
        contract_id: str,
        file_data: bytes,
        filename: str
    ) -> str:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞ —Ñ–∞–π–ª–∞"""
        if self.storage:
            return self.storage.store_original(
                contract_id, file_data, Path(filename).suffix
            )
        else:
            # Fallback: —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–∫–∞–ª—å–Ω–æ
            file_path = Path(f"data/contracts/originals/{contract_id}{Path(filename).suffix}")
            file_path.parent.mkdir(parents=True, exist_ok=True)
            file_path.write_bytes(file_data)
            return str(file_path)

    def _convert_pdf_to_images(self, pdf_path: str) -> List[Any]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è PDF –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        from pdf2image import convert_from_path

        try:
            images = convert_from_path(pdf_path, dpi=300)
            return images
        except Exception as e:
            logger.error(f"PDF to images conversion failed: {e}")
            raise

    def _extract_text_from_ocr(
        self,
        ocr_result,
        bbox: tuple
    ) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ OCR —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –ø–æ bbox"""
        # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ bbox
        return ocr_result.text if hasattr(ocr_result, 'text') else ""

    def _xml_to_intermediate(self, xml_data: str) -> Dict[str, Any]:
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ XML ‚Üí Intermediate JSON"""
        from lxml import etree
        from ..utils.xml_security import parse_xml_safely

        try:
            root = parse_xml_safely(xml_data)

            intermediate = {
                'doc_number': root.findtext('.//doc_number', default='UNKNOWN'),
                'signed_date': root.findtext('.//signed_date'),
                'total_amount': root.findtext('.//total_amount'),
                'currency': root.findtext('.//currency', default='RUB'),
                'parties': [],
                'items': [],
                'payment_schedule': [],
                'rules': [],
                'attributes': {}
            }

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç–æ—Ä–æ–Ω—ã
            for party_elem in root.findall('.//party'):
                party = {
                    'role': party_elem.get('role', 'unknown'),
                    'name': party_elem.findtext('name', ''),
                    'inn': party_elem.findtext('inn'),
                    'legal_address': party_elem.findtext('address')
                }
                intermediate['parties'].append(party)

            # TODO: –ò–∑–≤–ª–µ—á—å –æ—Å—Ç–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ XML

            return intermediate

        except Exception as e:
            logger.error(f"XML to intermediate conversion failed: {e}")
            raise

    def _log_stage(
        self,
        contract_id: str,
        stage: str,
        status: str,
        output_data: Dict[str, Any],
        duration_ms: float,
        error_message: Optional[str] = None,
        tokens_used: Optional[int] = None,
        cost_usd: Optional[float] = None
    ):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —ç—Ç–∞–ø–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ –ë–î"""
        from ..models.idp_models import IDPExtractionLog

        log_entry = IDPExtractionLog(
            contract_id=contract_id,
            stage=stage,
            status=status,
            output_data=output_data,
            error_message=error_message,
            duration_ms=int(duration_ms),
            tokens_used=tokens_used,
            cost_usd=cost_usd,
            created_at=datetime.now()
        )

        self.db.add(log_entry)
        self.db.commit()

        logger.info(f"üìù Logged stage: {stage} ({status}) - {duration_ms:.0f}ms")

    def _create_quality_issues(
        self,
        contract_id: str,
        validation_error: Exception
    ):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–ø–∏—Å–µ–π –æ –ø—Ä–æ–±–ª–µ–º–∞—Ö –∫–∞—á–µ—Å—Ç–≤–∞"""
        from ..models.idp_models import IDPQualityIssue

        # TODO: –ü–∞—Ä—Å–∏—Ç—å validation_error –∏ —Å–æ–∑–¥–∞–≤–∞—Ç—å –¥–µ—Ç–∞–ª—å–Ω—ã–µ issues

        issue = IDPQualityIssue(
            contract_id=contract_id,
            issue_type='validation_error',
            severity='warning',
            description=str(validation_error),
            requires_manual_review=True,
            status='open'
        )

        self.db.add(issue)
        self.db.commit()


# –≠–∫—Å–ø–æ—Ä—Ç
__all__ = ['IDPOrchestrator']
