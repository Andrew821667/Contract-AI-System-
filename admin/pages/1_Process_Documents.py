"""
–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ - "–°—Ç–µ–∫–ª—è–Ω–Ω—ã–π —è—â–∏–∫"
–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –í–°–ï –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏
"""

import streamlit as st
import sys
from pathlib import Path
import asyncio
import json
import os
import tempfile
import pandas as pd
from typing import Dict, Any

# Add project root to path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

st.set_page_config(
    page_title="–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ - Contract AI",
    page_icon="üìÑ",
    layout="wide"
)

st.title("üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
st.markdown("**–°—Ç–µ–∫–ª—è–Ω–Ω—ã–π —è—â–∏–∫:** –≤–∏–¥–Ω—ã –≤—Å–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–±–æ—Ç—ã —Å–∏—Å—Ç–µ–º—ã")

st.markdown("---")

# –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞
st.header("1Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞")

uploaded_file = st.file_uploader(
    "–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª –¥–æ–≥–æ–≤–æ—Ä–∞",
    type=['pdf', 'docx', 'txt', 'png', 'jpg', 'jpeg'],
    help="–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è: PDF, DOCX, TXT, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (—Å OCR)"
)

# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è async –æ–±—Ä–∞–±–æ—Ç–∫–∏
async def process_document_async(file_path, file_ext):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    from src.services.document_processor import DocumentProcessor
    import os
    from dotenv import load_dotenv

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env —Ñ–∞–π–ª–∞
    load_dotenv()

    openai_api_key = os.getenv("OPENAI_API_KEY")
    if not openai_api_key:
        raise ValueError(
            "OPENAI_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è.\n"
            "–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª .env –≤ –∫–æ—Ä–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞ –∏ –¥–æ–±–∞–≤—å—Ç–µ: OPENAI_API_KEY=your_key_here"
        )

    processor = DocumentProcessor(
        openai_api_key=openai_api_key,
        use_rag=True,
        use_section_analysis=True  # –í–∫–ª—é—á–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–∞–∑–¥–µ–ª–æ–≤
    )
    result = await processor.process_document(file_path, file_ext)
    return result


def get_optimal_model_info(stage: str) -> tuple[str, str]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —ç—Ç–∞–ø–∞"""
    models = {
        "text_extraction": (
            "N/A (–ø—Ä—è–º–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ)",
            "pdfplumber + PaddleOCR –¥–ª—è —Å–∫–∞–Ω–æ–≤ + LayoutLMv3 –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –º–∞–∫–µ—Ç–æ–≤"
        ),
        "level1": (
            "regex + SpaCy (ru_core_news_sm)",
            "SpaCy ru_core_news_lg, DeepPavlov NER, –∏–ª–∏ Qwen2.5-VL-72B –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"
        ),
        "llm": (
            "gpt-4o-mini ($0.15/1M –≤—Ö–æ–¥)",
            "Claude 4.5 Sonnet ($3/1M) –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –¥–æ–≥–æ–≤–æ—Ä–æ–≤, DeepSeek-R1 ($0.28/1M) –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏, Qwen3-235B-A22B –¥–ª—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö"
        ),
        "rag": (
            "pgvector + text-embedding-ada-002",
            "OpenAI text-embedding-3-large –∏–ª–∏ Cohere embed-multilingual-v3.0 –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ –∏ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –¥–æ–≥–æ–≤–æ—Ä–æ–≤"
        ),
        "validation": (
            "Business rules + Pydantic",
            "Claude 4.5 Sonnet ($3/1M) –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞, Qwen3-235B-A22B –¥–ª—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤"
        )
    }
    return models.get(stage, ("N/A", "N/A"))


def display_validation_section(section_analysis_data: Dict[str, Any]):
    """–£–°–¢–ê–†–ï–í–®–ê–Ø —Ñ—É–Ω–∫—Ü–∏—è - –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ display_validation_section_dynamic()"""
    # –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –±–æ–ª—å—à–µ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
    # –í—ã–∑—ã–≤–∞–µ—Ç—Å—è display_validation_section_dynamic() –≤ —Å—Ç—Ä–æ–∫–µ 656
    pass


def extract_section_text(full_text: str, start_marker: str, end_marker: str) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞ –¥–æ–≥–æ–≤–æ—Ä–∞"""
    try:
        start_idx = full_text.find(start_marker)
        end_idx = full_text.find(end_marker)

        if start_idx == -1:
            return "–†–∞–∑–¥–µ–ª –Ω–µ –Ω–∞–π–¥–µ–Ω"

        if end_idx == -1:
            # –ï—Å–ª–∏ —ç—Ç–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–∞–∑–¥–µ–ª
            return full_text[start_idx:start_idx + 500]

        return full_text[start_idx:end_idx].strip()
    except:
        return "–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ —Ä–∞–∑–¥–µ–ª–∞"


# –ö–Ω–æ–ø–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
if uploaded_file is not None:
    st.success(f"‚úÖ –§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω: **{uploaded_file.name}** ({uploaded_file.size} –±–∞–π—Ç)")

    if st.button("üöÄ –ù–∞—á–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É", type="primary"):
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
        with tempfile.NamedTemporaryFile(delete=False, suffix=Path(uploaded_file.name).suffix) as tmp_file:
            tmp_file.write(uploaded_file.read())
            tmp_file_path = tmp_file.name

        try:
            # Progress bar
            progress_bar = st.progress(0)
            status_text = st.empty()

            # –ó–∞–ø—É—Å–∫–∞–µ–º —Ä–µ–∞–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É
            status_text.text("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏...")
            progress_bar.progress(5)

            # –ó–∞–ø—É—Å–∫–∞–µ–º async –æ–±—Ä–∞–±–æ—Ç–∫—É
            result = asyncio.run(process_document_async(
                tmp_file_path,
                Path(uploaded_file.name).suffix
            ))

            st.markdown("---")
            st.header("2Ô∏è‚É£ –•–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏")

            # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–∞–∂–¥–æ–≥–æ —ç—Ç–∞–ø–∞
            total_stages = len(result.stages)

            for idx, stage in enumerate(result.stages):
                progress = int((idx + 1) / total_stages * 90)
                progress_bar.progress(progress)

                # Stage 1: Text Extraction
                if stage.name == "text_extraction":
                    status_text.text("üìÑ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞...")

                    with st.expander(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ ({stage.duration_sec:.1f} —Å–µ–∫)", expanded=True):
                        used_model, optimal_model = get_optimal_model_info("text_extraction")
                        st.success(f"**–ú–µ—Ç–æ–¥:** {stage.results.get('method', 'N/A')}")
                        st.info(f"**–ú–æ–¥–µ–ª—å:** {used_model} | **–û–ø—Ç–∏–º–∞–ª—å–Ω–æ:** {optimal_model}")

                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("–°—Ç—Ä–∞–Ω–∏—Ü", stage.results.get("pages", "N/A"))
                        with col2:
                            st.metric("–°–∏–º–≤–æ–ª–æ–≤", f"{stage.results.get('chars', 0):,}")
                        with col3:
                            confidence = stage.results.get("confidence")
                            st.metric("Confidence", f"{confidence:.2f}" if confidence else "N/A")

                        st.subheader("üìã –ü–û–õ–ù–´–ô –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç")
                        st.text_area("–í–µ—Å—å —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–ø—Ä–æ–∫—Ä—É—Ç–∏—Ç–µ –≤–Ω–∏–∑):", value=result.raw_text, height=400, key="full_text_area")

                # Stage 2: Level 1 Extraction
                elif stage.name == "level1_extraction":
                    status_text.text("üîç Level 1: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π...")

                    with st.expander(f"‚úÖ Level 1 Extraction ({stage.duration_sec:.1f} —Å–µ–∫)", expanded=True):
                        used_model, optimal_model = get_optimal_model_info("level1")
                        st.success(f"**–ù–∞–π–¥–µ–Ω–æ —Å—É—â–Ω–æ—Å—Ç–µ–π:** {stage.results.get('entities_count', 0)}")
                        st.info(f"**–ú–æ–¥–µ–ª—å:** {used_model} | **–û–ø—Ç–∏–º–∞–ª—å–Ω–æ:** {optimal_model}")

                        # –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ —Ç–∏–ø–∞–º
                        by_type = stage.results.get("by_type", {})
                        cols = st.columns(min(len(by_type), 3))
                        for idx, (entity_type, count) in enumerate(by_type.items()):
                            with cols[idx % 3]:
                                st.metric(entity_type, count)

                        # –î–µ—Ç–∞–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
                        st.subheader("üìã –î–µ—Ç–∞–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Å—É—â–Ω–æ—Å—Ç–µ–π")
                        details = stage.results.get("details", {})

                        all_entities = []
                        for entity_type, entities in details.items():
                            for ent in entities:
                                all_entities.append({
                                    "–¢–∏–ø": entity_type,
                                    "–ó–Ω–∞—á–µ–Ω–∏–µ": ent.get("value", ""),
                                    "Confidence": f"{ent.get('confidence', 0):.2f}",
                                    "–ö–æ–Ω—Ç–µ–∫—Å—Ç": ent.get("context", "")[:80] + "..."
                                })

                        if all_entities:
                            st.dataframe(all_entities, use_container_width=True)

                # Stage 3: LLM Extraction
                elif stage.name == "llm_extraction":
                    status_text.text("ü§ñ LLM –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")

                    with st.expander(f"‚úÖ LLM Extraction ({stage.duration_sec:.1f} —Å–µ–∫)", expanded=True):
                        model_used = stage.results.get("model", "N/A")
                        used_model, optimal_model = get_optimal_model_info("llm")

                        st.success(f"**–ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞:** {model_used}")
                        st.info(f"**–û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å:** {optimal_model}")

                        # –ú–µ—Ç—Ä–∏–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
                        st.subheader("üìä –ú–µ—Ç—Ä–∏–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
                        tokens_in = stage.results.get("tokens_input", 0)
                        tokens_out = stage.results.get("tokens_output", 0)
                        cost = stage.results.get("cost_usd", 0)
                        confidence = stage.results.get("confidence", 0)

                        metrics_data = [
                            {"–ü–∞—Ä–∞–º–µ—Ç—Ä": "–¢–æ–∫–µ–Ω—ã (–≤—Ö–æ–¥)", "–ó–Ω–∞—á–µ–Ω–∏–µ": f"{tokens_in:,}", "–û–ø–∏—Å–∞–Ω–∏–µ": "–¢–æ–∫–µ–Ω–æ–≤ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –≤ –º–æ–¥–µ–ª—å"},
                            {"–ü–∞—Ä–∞–º–µ—Ç—Ä": "–¢–æ–∫–µ–Ω—ã (–≤—ã—Ö–æ–¥)", "–ó–Ω–∞—á–µ–Ω–∏–µ": f"{tokens_out:,}", "–û–ø–∏—Å–∞–Ω–∏–µ": "–¢–æ–∫–µ–Ω–æ–≤ –ø–æ–ª—É—á–µ–Ω–æ –æ—Ç –º–æ–¥–µ–ª–∏"},
                            {"–ü–∞—Ä–∞–º–µ—Ç—Ä": "–í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤", "–ó–Ω–∞—á–µ–Ω–∏–µ": f"{tokens_in + tokens_out:,}", "–û–ø–∏—Å–∞–Ω–∏–µ": "–°—É–º–º–∞—Ä–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ"},
                            {"–ü–∞—Ä–∞–º–µ—Ç—Ä": "–°—Ç–æ–∏–º–æ—Å—Ç—å", "–ó–Ω–∞—á–µ–Ω–∏–µ": f"${cost:.5f}", "–û–ø–∏—Å–∞–Ω–∏–µ": f"{model_used}: —Å–º. —Ç–∞—Ä–∏—Ñ—ã –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞"},
                            {"–ü–∞—Ä–∞–º–µ—Ç—Ä": "Confidence", "–ó–Ω–∞—á–µ–Ω–∏–µ": f"{confidence:.2f} ({confidence*100:.0f}%)", "–û–ø–∏—Å–∞–Ω–∏–µ": "–°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏"},
                        ]
                        st.table(metrics_data)

                        # –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                        st.subheader("üìä –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
                        extracted_data = stage.results.get("data", {})

                        tab1, tab2, tab3, tab4, tab5 = st.tabs(["–°—Ç–æ—Ä–æ–Ω—ã", "–ü—Ä–µ–¥–º–µ—Ç", "–§–∏–Ω–∞–Ω—Å—ã", "–°—Ä–æ–∫–∏", "–°–∞–Ω–∫—Ü–∏–∏"])

                        with tab1:
                            st.json(extracted_data.get("parties", {}))

                        with tab2:
                            st.json(extracted_data.get("subject", {}))

                        with tab3:
                            st.json(extracted_data.get("financials", {}))

                        with tab4:
                            st.json(extracted_data.get("terms", {}))

                        with tab5:
                            st.json(extracted_data.get("penalties", {}))

                # Stage 4: RAG Filter
                elif stage.name == "rag_filter":
                    status_text.text("üîç RAG: –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–≥–æ–≤–æ—Ä–æ–≤...")

                    with st.expander(f"‚úÖ RAG Filter ({stage.duration_sec:.1f} —Å–µ–∫)", expanded=False):
                        used_model, optimal_model = get_optimal_model_info("rag")
                        similar_count = stage.results.get("similar_contracts_found", 0)

                        st.success(f"**–ù–∞–π–¥–µ–Ω–æ –ø–æ—Ö–æ–∂–∏—Ö:** {similar_count} –¥–æ–≥–æ–≤–æ—Ä–æ–≤")
                        st.info(f"**–ú–æ–¥–µ–ª—å:** {used_model} | **–û–ø—Ç–∏–º–∞–ª—å–Ω–æ:** {optimal_model}")

                        contracts = stage.results.get("contracts", [])
                        if contracts:
                            similar_data = []
                            for c in contracts:
                                similar_data.append({
                                    "–î–æ–≥–æ–≤–æ—Ä": c.get("contract_number", "N/A"),
                                    "–°—Ö–æ–∂–µ—Å—Ç—å": f"{c.get('similarity', 0):.2f}",
                                    "–¢–∏–ø": c.get("doc_type", "N/A"),
                                    "–°—É–º–º–∞": f"‚ÇΩ{c.get('amount', 0):,.0f}"
                                })
                            st.dataframe(similar_data, use_container_width=True)
                        else:
                            st.info("–ü–æ—Ö–æ–∂–∏–µ –¥–æ–≥–æ–≤–æ—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã (–±–∞–∑–∞ –ø—É—Å—Ç–∞ –∏–ª–∏ –Ω–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π)")

            # Stage 5: Validation
            progress_bar.progress(95)
            status_text.text("‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")

            validation_result = result.validation_result or {}

            with st.expander("‚ö†Ô∏è Validation", expanded=True):
                used_model, optimal_model = get_optimal_model_info("validation")

                validation_status = validation_result.get("status", "unknown")
                if validation_status == "passed":
                    st.success("**–°—Ç–∞—Ç—É—Å:** ‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –ø—Ä–æ–π–¥–µ–Ω–∞")
                elif validation_status == "passed_with_warnings":
                    st.warning("**–°—Ç–∞—Ç—É—Å:** ‚ö†Ô∏è –í–∞–ª–∏–¥–∞—Ü–∏—è –ø—Ä–æ–π–¥–µ–Ω–∞ —Å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è–º–∏")
                else:
                    st.error("**–°—Ç–∞—Ç—É—Å:** ‚ùå –í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–µ –ø—Ä–æ–π–¥–µ–Ω–∞")

                st.info(f"**–ú–æ–¥–µ–ª—å:** {used_model} | **–û–ø—Ç–∏–º–∞–ª—å–Ω–æ:** {optimal_model}")

                errors = validation_result.get("errors", [])
                warnings = validation_result.get("warnings", [])

                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("–û—à–∏–±–æ–∫", len(errors), delta="‚úÖ" if len(errors) == 0 else "‚ùå")
                with col2:
                    st.metric("–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π", len(warnings), delta="‚ö†Ô∏è" if len(warnings) > 0 else "‚úÖ")
                with col3:
                    compliance = 100 - (len(errors) * 10 + len(warnings) * 2)
                    st.metric("–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ", f"{compliance}%", delta=f"{compliance-100}%" if compliance < 100 else "‚úÖ")

                st.markdown("---")

                # –î–µ—Ç–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –ø–æ —Ä–∞–∑–¥–µ–ª–∞–º (–î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò –∏–∑ LLM)
                section_analysis_data = None
                for stage in result.stages:
                    if stage.name == "section_analysis" and stage.status == "success":
                        section_analysis_data = stage.results.get("full_data")
                        break

                if section_analysis_data:
                    display_validation_section_dynamic(section_analysis_data)
                else:
                    st.warning("‚ö†Ô∏è –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–∞–∑–¥–µ–ª–æ–≤ –Ω–µ –±—ã–ª –≤—ã–ø–æ–ª–Ω–µ–Ω. –í–æ–∑–º–æ–∂–Ω–æ, use_section_analysis=False –∏–ª–∏ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞.")

            progress_bar.progress(100)
            status_text.empty()

            st.markdown("---")

            # –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            st.header("3Ô∏è‚É£ –ò—Ç–æ–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏")

            col1, col2, col3, col4 = st.columns(4)

            with col1:
                st.metric("‚è±Ô∏è –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏", f"{result.total_time_sec:.1f} —Å–µ–∫")

            with col2:
                st.metric("üí∞ –°—Ç–æ–∏–º–æ—Å—Ç—å", f"${result.total_cost_usd:.5f}")

            with col3:
                st.metric("ü§ñ –ú–æ–¥–µ–ª—å", result.model_used)

            with col4:
                avg_confidence = 0
                for stage in result.stages:
                    if stage.name == "llm_extraction":
                        avg_confidence = stage.results.get("confidence", 0)
                st.metric("üéØ –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å", f"{avg_confidence*100:.0f}%")

            st.markdown("---")

            # –ö–Ω–æ–ø–∫–∏ –¥–µ–π—Å—Ç–≤–∏–π
            st.header("4Ô∏è‚É£ –î–µ–π—Å—Ç–≤–∏—è —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏")

            col1, col2, col3, col4 = st.columns(4)

            with col1:
                if st.button("‚úÖ –£—Ç–≤–µ—Ä–¥–∏—Ç—å", type="primary", use_container_width=True):
                    st.success("‚úÖ –î–æ–∫—É–º–µ–Ω—Ç —É—Ç–≤–µ—Ä–∂–¥–µ–Ω –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö!")
                    st.balloons()

            with col2:
                if st.button("üíæ –°–æ—Ö—Ä–∞–Ω–∏—Ç—å JSON", use_container_width=True):
                    json_data = json.dumps(result.to_dict(), ensure_ascii=False, indent=2)
                    st.download_button(
                        "–°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç",
                        json_data,
                        file_name=f"contract_analysis_{uploaded_file.name}.json",
                        mime="application/json"
                    )

            with col3:
                if st.button("üìÑ –≠–∫—Å–ø–æ—Ä—Ç –≤ Word", use_container_width=True):
                    st.info("–≠–∫—Å–ø–æ—Ä—Ç –≤ Word (–≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ)")

            with col4:
                if st.button("‚ùå –û—Ç–∫–ª–æ–Ω–∏—Ç—å", use_container_width=True):
                    st.error("–î–æ–∫—É–º–µ–Ω—Ç –æ—Ç–∫–ª–æ–Ω–µ–Ω")

        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}")
            import traceback
            st.code(traceback.format_exc())

        finally:
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            if os.path.exists(tmp_file_path):
                os.unlink(tmp_file_path)

else:
    st.info("üëÜ –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª –¥–æ–≥–æ–≤–æ—Ä–∞ –¥–ª—è –Ω–∞—á–∞–ª–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏")

    # –°—Å—ã–ª–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª
    st.markdown("---")
    st.markdown("**üí° –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:** –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ñ–∞–π–ª `tests/fixtures/test_supply_contract.txt`")

st.markdown("---")
st.caption("Contract AI System v2.0 - –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ | –ú–æ–¥–µ–ª–∏: Claude 4.5 Sonnet, DeepSeek-R1, Qwen3-235B, GPT-4o")


def display_validation_section_dynamic(section_analysis_data: Dict[str, Any]):
    """–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –≤–∞–ª–∏–¥–∞—Ü–∏—é –ø–æ —Ä–∞–∑–¥–µ–ª–∞–º –¥–æ–≥–æ–≤–æ—Ä–∞ (–î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò –∏–∑ LLM)"""

    if not section_analysis_data:
        st.warning("–ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–¥–µ–ª–æ–≤ –Ω–µ –±—ã–ª –≤—ã–ø–æ–ª–Ω–µ–Ω")
        return

    st.subheader("üìã –î–µ—Ç–∞–ª—å–Ω—ã–π —Ä–∞–∑–±–æ—Ä –ø–æ —Ä–∞–∑–¥–µ–ª–∞–º –¥–æ–≥–æ–≤–æ—Ä–∞")

    sections = section_analysis_data.get("sections", [])
    section_analyses = section_analysis_data.get("section_analyses", [])
    complex_analysis = section_analysis_data.get("complex_analysis")

    if not sections:
        st.warning("–†–∞–∑–¥–µ–ª—ã –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã –≤ –¥–æ–≥–æ–≤–æ—Ä–µ")
        return

    st.info(f"**–ù–∞–π–¥–µ–Ω–æ —Ä–∞–∑–¥–µ–ª–æ–≤:** {len(sections)} | **–ü–æ—Ä—è–¥–æ–∫ –ø—Ä–æ–≤–µ—Ä–∫–∏:** 1Ô∏è‚É£ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –¥–æ–≥–æ–≤–æ—Ä–∞–º–∏ ‚Üí 2Ô∏è‚É£ –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ RAG –±–∞–∑–µ (–∞–∫—Ç—É–∞–ª—å–Ω–∞—è –ø—Ä–∞–≤–æ–≤–∞—è –±–∞–∑–∞) ‚Üí 3Ô∏è‚É£ –§–æ–ª–±—ç–∫ –Ω–∞ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –º–æ–¥–µ–ª–∏")

    # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞–µ–º –≤–∫–ª–∞–¥–∫–∏
    tab_names = [f"–†–∞–∑–¥–µ–ª {s.number}" for s in sections] + ["üîç –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑"]
    tabs = st.tabs(tab_names)

    # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –∫–∞–∂–¥—ã–π —Ä–∞–∑–¥–µ–ª –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò
    for idx, (section, analysis) in enumerate(zip(sections, section_analyses)):
        with tabs[idx]:
            st.markdown(f"### üìÑ –†–∞–∑–¥–µ–ª {section.number}: {section.title}")

            # –¢–µ–∫—Å—Ç —Ä–∞–∑–¥–µ–ª–∞
            st.text_area("–¢–µ–∫—Å—Ç —Ä–∞–∑–¥–µ–ª–∞:", section.text, height=150, key=f"section_{section.number}_text")

            st.markdown("---")

            # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –¥–æ–≥–æ–≤–æ—Ä–∞–º–∏
            st.markdown("**1Ô∏è‚É£ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –¥–æ–≥–æ–≤–æ—Ä–∞–º–∏:**")
            if analysis.own_contracts_comparison.startswith("‚úÖ"):
                st.success(analysis.own_contracts_comparison)
            elif analysis.own_contracts_comparison.startswith("‚ö†Ô∏è"):
                st.warning(analysis.own_contracts_comparison)
            else:
                st.error(analysis.own_contracts_comparison)

            # –î–µ—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
            if analysis.own_contracts_details:
                st.dataframe(analysis.own_contracts_details, use_container_width=True)

            # RAG –ø—Ä–æ–≤–µ—Ä–∫–∞
            st.markdown("**2Ô∏è‚É£ –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ RAG (–∞–∫—Ç—É–∞–ª—å–Ω–∞—è –ø—Ä–∞–≤–æ–≤–∞—è –±–∞–∑–∞):**")
            st.info(analysis.rag_legal_check)

            if analysis.rag_legal_references:
                st.markdown("**–°—Å—ã–ª–∫–∏ –Ω–∞ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ:**")
                for ref in analysis.rag_legal_references:
                    st.markdown(f"- {ref}")

            st.markdown("---")

            # –í—ã–≤–æ–¥—ã –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            if analysis.conclusion.startswith("–†–∞–∑–¥–µ–ª –ø—Ä–æ—Ä–∞–±–æ—Ç–∞–Ω —Ö–æ—Ä–æ—à–æ") or "—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç" in analysis.conclusion.lower():
                st.success(f"**–í—ã–≤–æ–¥:** {analysis.conclusion}")
            elif "—Ç—Ä–µ–±—É–µ—Ç" in analysis.conclusion.lower() or "–¥–æ—Ä–∞–±–æ—Ç–∫" in analysis.conclusion.lower():
                st.warning(f"**–í—ã–≤–æ–¥:** {analysis.conclusion}")
            else:
                st.info(f"**–í—ã–≤–æ–¥:** {analysis.conclusion}")

            if analysis.warnings:
                st.markdown("**‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è:**")
                for warning in analysis.warnings:
                    st.warning(warning)

            if analysis.recommendations:
                st.markdown("**üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**")
                for rec in analysis.recommendations:
                    st.info(rec)

    # –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (–ø–æ—Å–ª–µ–¥–Ω—è—è –≤–∫–ª–∞–¥–∫–∞)
    with tabs[-1]:
        st.markdown("### üîç –ö–û–ú–ü–õ–ï–ö–°–ù–´–ô –ê–ù–ê–õ–ò–ó –î–û–ì–û–í–û–†–ê")
        st.markdown("–ê–Ω–∞–ª–∏–∑ –≤–∑–∞–∏–º–æ—Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Ä–∞–∑–¥–µ–ª–∞–º–∏ –∏ –æ–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞")

        if not complex_analysis:
            st.warning("–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω")
            return

        st.markdown("---")
        st.markdown("#### 1Ô∏è‚É£ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏")
        if complex_analysis.integrity_checks:
            st.dataframe(complex_analysis.integrity_checks, use_container_width=True)

        st.markdown("---")
        st.markdown("#### 2Ô∏è‚É£ –Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ —Ä–∏—Å–∫–∏")

        risk_col1, risk_col2, risk_col3 = st.columns(3)

        with risk_col1:
            st.markdown("**üü¢ –ù–ò–ó–ö–ò–ô –†–ò–°–ö:**")
            for risk in complex_analysis.risk_assessment.get("low", []):
                st.success(f"‚úÖ {risk}")

        with risk_col2:
            st.markdown("**üü° –°–†–ï–î–ù–ò–ô –†–ò–°–ö:**")
            for risk in complex_analysis.risk_assessment.get("medium", []):
                st.warning(f"‚ö†Ô∏è {risk}")

        with risk_col3:
            st.markdown("**üî¥ –í–´–°–û–ö–ò–ô –†–ò–°–ö:**")
            for risk in complex_analysis.risk_assessment.get("high", []):
                st.error(f"‚ùå {risk}")

        st.markdown("---")
        st.markdown("#### 3Ô∏è‚É£ –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤—É –†–§")
        if complex_analysis.legal_compliance:
            st.dataframe(complex_analysis.legal_compliance, use_container_width=True)

        st.markdown("---")
        st.markdown("#### 4Ô∏è‚É£ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –ª—É—á—à–∏–º–∏ –ø—Ä–∞–∫—Ç–∏–∫–∞–º–∏")
        st.info("**–ò—Å—Ç–æ—á–Ω–∏–∫:** –ê–Ω–∞–ª–∏–∑ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–≥–æ–≤–æ—Ä–æ–≤ –∏–∑ –±–∞–∑—ã + RAG –∞–∫—Ç—É–∞–ª—å–Ω–∞—è –ø—Ä–∞–≤–æ–≤–∞—è –±–∞–∑–∞ + –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –º–æ–¥–µ–ª–∏")
        if complex_analysis.best_practices:
            st.dataframe(complex_analysis.best_practices, use_container_width=True)

        st.markdown("---")
        st.markdown("#### 5Ô∏è‚É£ –ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏")

        score_col1, score_col2, score_col3 = st.columns(3)

        with score_col1:
            st.metric("–û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞", f"{complex_analysis.overall_score}/100",
                      delta="–•–æ—Ä–æ—à–æ" if complex_analysis.overall_score >= 80 else "–¢—Ä–µ–±—É–µ—Ç –¥–æ—Ä–∞–±–æ—Ç–∫–∏")

        with score_col2:
            st.metric("–Æ—Ä–∏–¥–∏—á–µ—Å–∫–∞—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å", f"{complex_analysis.legal_reliability:.1f}/10",
                      delta="–í—ã—Å–æ–∫–∞—è" if complex_analysis.legal_reliability >= 8 else "–°—Ä–µ–¥–Ω—è—è")

        with score_col3:
            st.metric("–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∑–∞–∫–æ–Ω—É", f"{complex_analysis.compliance_percent}%",
                      delta=f"+{100 - complex_analysis.compliance_percent}% –ø–æ—Å–ª–µ –¥–æ—Ä–∞–±–æ—Ç–∫–∏")

        st.markdown("---")

        rec_col1, rec_col2 = st.columns(2)

        with rec_col1:
            st.markdown("**‚úÖ –°–ò–õ–¨–ù–´–ï –°–¢–û–†–û–ù–´:**")
            for strength in complex_analysis.strengths:
                st.success(strength)

        with rec_col2:
            st.markdown("**‚ö†Ô∏è –ö–†–ò–¢–ò–ß–ù–´–ï –î–û–†–ê–ë–û–¢–ö–ò:**")
            for improvement in complex_analysis.critical_improvements:
                if improvement.startswith("–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û") or improvement.startswith("–ö–†–ò–¢–ò–ß–ù–û"):
                    st.error(improvement)
                else:
                    st.warning(improvement)

        st.markdown("---")
        avg_score = complex_analysis.overall_score
        if avg_score >= 90:
            st.success("**üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** –î–æ–≥–æ–≤–æ—Ä –≥–æ—Ç–æ–≤ –∫ –ø–æ–¥–ø–∏—Å–∞–Ω–∏—é. –û—Ç–ª–∏—á–Ω–∞—è –ø—Ä–æ—Ä–∞–±–æ—Ç–∫–∞!")
        elif avg_score >= 80:
            st.info("**üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** –î–æ–≥–æ–≤–æ—Ä –º–æ–∂–Ω–æ –ø–æ–¥–ø–∏—Å—ã–≤–∞—Ç—å –ø–æ—Å–ª–µ –≤–Ω–µ—Å–µ–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã—Ö –¥–æ—Ä–∞–±–æ—Ç–æ–∫.")
        elif avg_score >= 70:
            st.warning("**üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** –î–æ–≥–æ–≤–æ—Ä —Ç—Ä–µ–±—É–µ—Ç –¥–æ—Ä–∞–±–æ—Ç–æ–∫. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø—Ä–∞–≤–∏—Ç—å –∫—Ä–∏—Ç–∏—á–Ω—ã–µ –∑–∞–º–µ—á–∞–Ω–∏—è –ø–µ—Ä–µ–¥ –ø–æ–¥–ø–∏—Å–∞–Ω–∏–µ–º.")
        else:
            st.error("**üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** –î–æ–≥–æ–≤–æ—Ä —Ç—Ä–µ–±—É–µ—Ç —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –ø–µ—Ä–µ—Ä–∞–±–æ—Ç–∫–∏. –ù–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∫ –ø–æ–¥–ø–∏—Å–∞–Ω–∏—é –≤ —Ç–µ–∫—É—â–µ–º –≤–∏–¥–µ.")
